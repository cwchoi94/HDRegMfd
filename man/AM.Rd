% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AM.R
\name{AM}
\alias{AM}
\title{High-dimensional additive regression for manifold-valued responses and covariates.}
\usage{
AM(
  Xorg,
  Yorg,
  Yspace,
  degree = 0,
  penalty = "LASSO",
  gamma = 0,
  lambda = 0.1,
  Xdim.max = 100,
  R = 100,
  bandwidths.list = NULL,
  transform = "Gaussian",
  normalize = FALSE,
  ngrid = 51,
  Kdenom_method = "numeric",
  phi = 1,
  eta = 0.001,
  max.iter = 200,
  threshold = 1e-06,
  SBF.comp = NULL
)
}
\arguments{
\item{Xorg}{a list of covariates with the following components (see also \code{\link{covariates.generate}}):
\describe{
      \item{j}{a \eqn{p} list of manifold-valued covariates, where each \eqn{j}th element is an \eqn{n\times T_j} matrix.}
      \item{spaces}{a \eqn{p} vector of the underlying spaces \eqn{\mathcal{M}_j} of \eqn{X_j}, see \code{\link{Check.manifold}}.}
      \item{p}{the number of \eqn{X_j}.}
}}

\item{Yorg}{an \eqn{n\times m} matrix of manifold-valued responses.}

\item{Yspace}{the name of the underlying space \eqn{\mathcal{M}_Y} of \eqn{Y}.}

\item{degree}{the degree of the local polynomial fitting. Options are: 0 for Nadaraya-Watson (default), 1 for local-linear, etc.}

\item{penalty}{the name of a penalty function. This must be one of 'LASSO', 'SCAD', or 'MCP' (default: 'LASSO').}

\item{gamma}{a parameter for SCAD (default: 3.7) or MCP (default: 3).}

\item{lambda}{a non-negative penalty constant (default: 0.1).}

\item{Xdim.max}{the maximum dimension to which \eqn{X_j} will be reduced (default: 100).}

\item{R}{an \eqn{\ell^1}-type constrained bound, multiplied by \eqn{\hat\sigma_Y} (default: 100).}

\item{transform}{a method of transformation. Must be one of 'linear', 'Gaussian' or 'trigonometric' (default: 'linear').}

\item{normalize}{a parameter indicating whether to standardize each column of score matrices (default: \code{TRUE}).}

\item{ngrid}{the number of grid points for evaluating the KDE (default: 51).}

\item{Kdenom_method}{a method used for the denominator in the normalized kernel calculation.}

\item{phi}{a parameter for computing the ADMM-based algorithm for the majorized objective function (default: 1).}

\item{eta}{a parameter for computing the ADMM-based algorithm for the proximal norm square (default: 1e-3).}

\item{max.iter}{a maximum number of iterations (default: 200).}

\item{threshold}{a convergence threshold for the algorithm (default: 1e-6).}

\item{bandwidth.list}{a \eqn{p} list of bandwidths. If \code{bandwidth.list==NULL}, then we compute the bandwidths by \code{\link{get.bandwidths}}.}

\item{SBF.compe}{a \code{\link{SBF.comp}} object (default: \code{NULL}). If \code{SBF.comp==NULL}, this function computes the SBF.comp using \code{\link{SBF.comp}}.}
}
\value{
a \code{AM} object with the following compnents:
   \describe{
      \item{pca}{a \code{\link{PCA.manifold.list}} object.}
      \item{Ymu}{the Frechet mean \eqn{\mu_Y} of \eqn{Y}.}
      \item{Yspace}{the underlying space of of \eqn{Y}.}
      \item{kde.1d}{a \eqn{p} list of one-dimensional KDE where each element is (ngrid,degree+1,degree+1) array.}
      \item{X.vector}{a \eqn{p} list of the corresponding eigenvectors of \eqn{X_j}.}
      \item{transform}{a \code{\link{Transform.Score}} object.}
      \item{mhat}{a \eqn{p} list of estimated \eqn{\hat m_{jk}}, where each element is an \eqn{ngrid \times (degree+1) \times m} cube.}
      \item{mhat.norm}{a \eqn{p\times 4} matrix of norms of \eqn{\hat m_{jk}}, with columns c('index','j','k','mhat.norm').}
      \item{all.indices}{an index set used in the estimation.}
      \item{proper.ind.mat}{a subset of \code{mhat.norm} with \eqn{\mathcal{S}=\{(j,k): \hat{m_{jk}}\neq0\}}.}
      \item{runtime}{the running time (HH:MM:SS).}
      \item{runtime.second}{the running time (second).}
      \item{...}{other parameters.}
}
}
\description{
Estimate additive component maps using an ADMM-based algorithm.
This function supports 'LASSO', 'SCAD', or 'MCP' penalty functions.
An \eqn{\ell^1}-type constrained bound \code{R} is multiplied by \eqn{\hat\sigma_Y = \sqrt{n^{-1}\sum_{i=1}^n d^2(Y_i,\hat\mu_Y)}}.
}
